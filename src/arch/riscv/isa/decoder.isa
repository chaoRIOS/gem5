// -*- mode:c++ -*-

// Copyright (c) 2015 RISC-V Foundation
// Copyright (c) 2017 The University of Virginia
// Copyright (c) 2020 Barkhausen Institut
// Copyright (c) 2021 StreamComputing Corp
// All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are
// met: redistributions of source code must retain the above copyright
// notice, this list of conditions and the following disclaimer;
// redistributions in binary form must reproduce the above copyright
// notice, this list of conditions and the following disclaimer in the
// documentation and/or other materials provided with the distribution;
// neither the name of the copyright holders nor the names of its
// contributors may be used to endorse or promote products derived from
// this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

////////////////////////////////////////////////////////////////////
//
// The RISC-V ISA decoder
//

decode QUADRANT default Unknown::unknown() {
    0x0: decode COPCODE {
        0x0: CIAddi4spnOp::c_addi4spn({{
            imm = CIMM8<1:1> << 2 |
                  CIMM8<0:0> << 3 |
                  CIMM8<7:6> << 4 |
                  CIMM8<5:2> << 6;
        }}, {{
            if (machInst == 0)
                return std::make_shared<IllegalInstFault>("zero instruction",
                                                           machInst);
            Rp2 = sp + imm;
        }}, uint64_t);
        format CompressedLoad {
            0x1: c_fld({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                STATUS status = xc->readMiscReg(MISCREG_STATUS);
                if (status.fs == FPUStatus::OFF)
                    return std::make_shared<IllegalInstFault>("FPU is off",
                                                               machInst);

                Fp2_bits = Mem;
            }}, {{
                EA = Rp1 + offset;
            }});
            0x2: c_lw({{
                offset = CIMM2<1:1> << 2 |
                         CIMM3 << 3 |
                         CIMM2<0:0> << 6;
            }}, {{
                Rp2_sd = Mem_sw;
            }}, {{
                EA = Rp1 + offset;
            }});
            0x3: c_ld({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                Rp2_sd = Mem_sd;
            }}, {{
                EA = Rp1 + offset;
            }});
        }
        format CompressedStore {
            0x5: c_fsd({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                STATUS status = xc->readMiscReg(MISCREG_STATUS);
                if (status.fs == FPUStatus::OFF)
                    return std::make_shared<IllegalInstFault>("FPU is off",
                                                               machInst);

                Mem = Fp2_bits;
            }}, {{
                EA = Rp1 + offset;
            }});
            0x6: c_sw({{
                offset = CIMM2<1:1> << 2 |
                         CIMM3 << 3 |
                         CIMM2<0:0> << 6;
            }}, {{
                Mem_uw = Rp2_uw;
            }}, ea_code={{
                EA = Rp1 + offset;
            }});
            0x7: c_sd({{
                offset = CIMM3 << 3 | CIMM2 << 6;
            }}, {{
                    Mem_ud = Rp2_ud;
            }}, {{
                EA = Rp1 + offset;
            }});
        }
    }
    0x1: decode COPCODE {
        format CIOp {
            0x0: c_addi({{
                imm = CIMM5;
                if (CIMM1 > 0)
                    imm |= ~((uint64_t)0x1F);
            }}, {{
                if ((RC1 == 0) != (imm == 0)) {
                    if (RC1 == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x0", machInst);
                    } else { // imm == 0
                        return std::make_shared<IllegalInstFault>(
                                "immediate = 0", machInst);
                    }
                }
                Rc1_sd = Rc1_sd + imm;
            }});
            0x1: c_addiw({{
                imm = CIMM5;
                if (CIMM1 > 0)
                    imm |= ~((uint64_t)0x1F);
            }}, {{
                if (RC1 == 0) {
                    return std::make_shared<IllegalInstFault>(
                            "source reg x0", machInst);
                }
                Rc1_sd = (int32_t)Rc1_sd + imm;
            }});
            0x2: c_li({{
                imm = CIMM5;
                if (CIMM1 > 0)
                    imm |= ~((uint64_t)0x1F);
            }}, {{
                if (RC1 == 0) {
                    return std::make_shared<IllegalInstFault>(
                            "source reg x0", machInst);
                }
                Rc1_sd = imm;
            }});
            0x3: decode RC1 {
                0x2: c_addi16sp({{
                    imm = CIMM5<4:4> << 4 |
                          CIMM5<0:0> << 5 |
                          CIMM5<3:3> << 6 |
                          CIMM5<2:1> << 7;
                    if (CIMM1 > 0)
                        imm |= ~((int64_t)0x1FF);
                }}, {{
                    if (imm == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "immediate = 0", machInst);
                    }
                    sp_sd = sp_sd + imm;
                }});
                default: c_lui({{
                    imm = CIMM5 << 12;
                    if (CIMM1 > 0)
                        imm |= ~((uint64_t)0x1FFFF);
                }}, {{
                    if (RC1 == 0 || RC1 == 2) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x0", machInst);
                    }
                    if (imm == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "immediate = 0", machInst);
                    }
                    Rc1_sd = imm;
                }});
            }
        }
        0x4: decode CFUNCT2HIGH {
            format CIOp {
                0x0: c_srli({{
                    imm = CIMM5 | (CIMM1 << 5);
                }}, {{
                    if (imm == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "immediate = 0", machInst);
                    }
                    Rp1 = Rp1 >> imm;
                }}, uint64_t);
                0x1: c_srai({{
                    imm = CIMM5 | (CIMM1 << 5);
                }}, {{
                    if (imm == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "immediate = 0", machInst);
                    }
                    Rp1_sd = Rp1_sd >> imm;
                }}, uint64_t);
                0x2: c_andi({{
                    imm = CIMM5;
                    if (CIMM1 > 0)
                        imm |= ~((uint64_t)0x1F);
                }}, {{
                    Rp1 = Rp1 & imm;
                }}, uint64_t);
            }
            format CompressedROp {
                0x3: decode CFUNCT1 {
                    0x0: decode CFUNCT2LOW {
                        0x0: c_sub({{
                            Rp1 = Rp1 - Rp2;
                        }});
                        0x1: c_xor({{
                            Rp1 = Rp1 ^ Rp2;
                        }});
                        0x2: c_or({{
                            Rp1 = Rp1 | Rp2;
                        }});
                        0x3: c_and({{
                            Rp1 = Rp1 & Rp2;
                        }});
                    }
                    0x1: decode CFUNCT2LOW {
                        0x0: c_subw({{
                            Rp1_sd = (int32_t)Rp1_sd - Rp2_sw;
                        }});
                        0x1: c_addw({{
                            Rp1_sd = (int32_t)Rp1_sd + Rp2_sw;
                        }});
                    }
                }
            }
        }
        0x5: CJOp::c_j({{
            NPC = PC + imm;
        }}, IsDirectControl, IsUncondControl);
        format CBOp {
            0x6: c_beqz({{
                if (Rp1 == 0)
                    NPC = PC + imm;
                else
                    NPC = NPC;
            }}, IsDirectControl, IsCondControl);
            0x7: c_bnez({{
                if (Rp1 != 0)
                    NPC = PC + imm;
                else
                    NPC = NPC;
            }}, IsDirectControl, IsCondControl);
        }
    }
    0x2: decode COPCODE {
        0x0: CIOp::c_slli({{
            imm = CIMM5 | (CIMM1 << 5);
        }}, {{
            if (imm == 0) {
                return std::make_shared<IllegalInstFault>(
                        "immediate = 0", machInst);
            }
            if (RC1 == 0) {
                return std::make_shared<IllegalInstFault>(
                        "source reg x0", machInst);
            }
            Rc1 = Rc1 << imm;
        }}, uint64_t);
        format CompressedLoad {
            0x1: c_fldsp({{
                offset = CIMM5<4:3> << 3 |
                         CIMM1 << 5 |
                         CIMM5<2:0> << 6;
            }}, {{
                Fc1_bits = Mem;
            }}, {{
                EA = sp + offset;
            }});
            0x2: c_lwsp({{
                offset = CIMM5<4:2> << 2 |
                         CIMM1 << 5 |
                         CIMM5<1:0> << 6;
            }}, {{
                if (RC1 == 0) {
                    return std::make_shared<IllegalInstFault>(
                            "source reg x0", machInst);
                }
                Rc1_sd = Mem_sw;
            }}, {{
                EA = sp + offset;
            }});
            0x3: c_ldsp({{
                offset = CIMM5<4:3> << 3 |
                         CIMM1 << 5 |
                         CIMM5<2:0> << 6;
            }}, {{
                if (RC1 == 0) {
                    return std::make_shared<IllegalInstFault>(
                            "source reg x0", machInst);
                }
                Rc1_sd = Mem_sd;
            }}, {{
                EA = sp + offset;
            }});
        }
        0x4: decode CFUNCT1 {
            0x0: decode RC2 {
                0x0: Jump::c_jr({{
                    if (RC1 == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x0", machInst);
                    }
                    NPC = Rc1;
                }}, IsIndirectControl, IsUncondControl, IsCall);
                default: CROp::c_mv({{
                    if (RC1 == 0) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x0", machInst);
                    }
                    Rc1 = Rc2;
                }});
            }
            0x1: decode RC1 {
                0x0: SystemOp::c_ebreak({{
                    if (RC2 != 0) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x1", machInst);
                    }
                    return std::make_shared<BreakpointFault>(xc->pcState());
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                default: decode RC2 {
                    0x0: Jump::c_jalr({{
                        if (RC1 == 0) {
                            return std::make_shared<IllegalInstFault>(
                                    "source reg x0", machInst);
                        }
                        ra = NPC;
                        NPC = Rc1;
                    }}, IsIndirectControl, IsUncondControl, IsCall);
                    default: CompressedROp::c_add({{
                        Rc1_sd = Rc1_sd + Rc2_sd;
                    }});
                }
            }
        }
        format CompressedStore {
            0x5: c_fsdsp({{
                offset = CIMM6<5:3> << 3 |
                         CIMM6<2:0> << 6;
            }}, {{
                Mem_ud = Fc2_bits;
            }}, {{
                EA = sp + offset;
            }});
            0x6: c_swsp({{
                offset = CIMM6<5:2> << 2 |
                         CIMM6<1:0> << 6;
            }}, {{
                Mem_uw = Rc2_uw;
            }}, {{
                EA = sp + offset;
            }});
            0x7: c_sdsp({{
                offset = CIMM6<5:3> << 3 |
                         CIMM6<2:0> << 6;
            }}, {{
                Mem = Rc2;
            }}, {{
                EA = sp + offset;
            }});
        }
    }
    0x3: decode OPCODE {
        0x00: decode FUNCT3 {
            format Load {
                0x0: lb({{
                    Rd_sd = Mem_sb;
                }});
                0x1: lh({{
                    Rd_sd = Mem_sh;
                }});
                0x2: lw({{
                    Rd_sd = Mem_sw;
                }});
                0x3: ld({{
                    Rd_sd = Mem_sd;
                }});
                0x4: lbu({{
                    Rd = Mem_ub;
                }});
                0x5: lhu({{
                    Rd = Mem_uh;
                }});
                0x6: lwu({{
                    Rd = Mem_uw;
                }});
            }
        }

        0x01: decode FUNCT3 {
            format Load {
                0x2: flw({{
                    STATUS status = xc->readMiscReg(MISCREG_STATUS);
                    if (status.fs == FPUStatus::OFF)
                        return std::make_shared<IllegalInstFault>(
                                    "FPU is off", machInst);
                    freg_t fd;
                    fd = freg(f32(Mem_uw));
                    Fd_bits = fd.v;
                }}, inst_flags=FloatMemReadOp);
                0x3: fld({{
                    STATUS status = xc->readMiscReg(MISCREG_STATUS);
                    if (status.fs == FPUStatus::OFF)
                        return std::make_shared<IllegalInstFault>(
                                    "FPU is off", machInst);
                    freg_t fd;
                    fd = freg(f64(Mem));
                    Fd_bits = fd.v;
                }}, inst_flags=FloatMemReadOp);
            }
            format VectorMemoryLoadOp {
                0x0: decode MOP {
                    0x0: decode LUMOP {
                        0x0: decode NF {
                            0x0: vle8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1: vlseg2e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x2: vlseg3e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3: vlseg4e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x4: vlseg5e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x5: vlseg6e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x6: vlseg7e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7: vlseg8e8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vl1re8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1:vl2re8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3:vl4re8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7:vl8re8_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0xb: vlm_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x10: vle8ff_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vluxei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vluxseg2ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vluxseg3ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vluxseg4ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vluxseg5ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vluxseg6ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vluxseg7ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vluxseg8ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vlse8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vlsseg2e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vlsseg3e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vlsseg4e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vlsseg5e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vlsseg6e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vlsseg7e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vlsseg8e8_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vloxei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vloxseg2ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vloxseg3ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vloxseg4ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vloxseg5ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vloxseg6ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vloxseg7ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vloxseg8ei8_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                }
                0x5: decode MOP {
                    0x0: decode LUMOP {
                        0x0: decode NF {
                            0x0: vle16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1: vlseg2e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x2: vlseg3e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3: vlseg4e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x4: vlseg5e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x5: vlseg6e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x6: vlseg7e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7: vlseg8e16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vl1re16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1:vl2re16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3:vl4re16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7:vl8re16_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x10: vle16ff_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vluxei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vluxseg2ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vluxseg3ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vluxseg4ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vluxseg5ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vluxseg6ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vluxseg7ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vluxseg8ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vlse16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vlsseg2e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vlsseg3e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vlsseg4e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vlsseg5e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vlsseg6e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vlsseg7e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vlsseg8e16_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vloxei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vloxseg2ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vloxseg3ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vloxseg4ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vloxseg5ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vloxseg6ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vloxseg7ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vloxseg8ei16_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                }
                0x6: decode MOP {
                    0x0: decode LUMOP {
                        0x0: decode NF {
                            0x0: vle32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1: vlseg2e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x2: vlseg3e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3: vlseg4e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x4: vlseg5e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x5: vlseg6e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x6: vlseg7e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7: vlseg8e32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vl1re32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1:vl2re32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3:vl4re32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7:vl8re32_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x10: vle32ff_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vluxei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vluxseg2ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vluxseg3ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vluxseg4ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vluxseg5ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vluxseg6ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vluxseg7ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vluxseg8ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vlse32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vlsseg2e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vlsseg3e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vlsseg4e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vlsseg5e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vlsseg6e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vlsseg7e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vlsseg8e32_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vloxei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vloxseg2ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vloxseg3ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vloxseg4ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vloxseg5ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vloxseg6ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vloxseg7ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vloxseg8ei32_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                }
                0x7: decode MOP {
                    0x0: decode LUMOP {
                        0x0: decode NF {
                            0x0: vle64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1: vlseg2e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x2: vlseg3e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3: vlseg4e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x4: vlseg5e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x5: vlseg6e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x6: vlseg7e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7: vlseg8e64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vl1re64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x1:vl2re64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x3:vl4re64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                            0x7:vl8re64_v({{Rs1}},
                                VectorMemoryLoadOp, IsLoad, IsVector);
                        }
                        0x10: vle64ff_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vluxei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vluxseg2ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vluxseg3ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vluxseg4ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vluxseg5ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vluxseg6ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vluxseg7ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vluxseg8ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vlse64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vlsseg2e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vlsseg3e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vlsseg4e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vlsseg5e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vlsseg6e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vlsseg7e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vlsseg8e64_v({{Rs1,Rs2}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vloxei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x1: vloxseg2ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x2: vloxseg3ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x3: vloxseg4ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x4: vloxseg5ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x5: vloxseg6ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x6: vloxseg7ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                        0x7: vloxseg8ei64_v({{Rs1}},
                            VectorMemoryLoadOp, IsLoad, IsVector);
                    }
                }
            }
        }

        0x03: decode FUNCT3 {
            format FenceOp {
                0x0: fence({{
                }}, uint64_t, IsReadBarrier, IsWriteBarrier, No_OpClass);
                0x1: fence_i({{
                }}, uint64_t, IsNonSpeculative, IsSerializeAfter, No_OpClass);
            }
        }

        0x04: decode FUNCT3 {
            format IOp {
                0x0: addi({{
                    Rd_sd = Rs1_sd + imm;
                }});
                0x1: slli({{
                    Rd = Rs1 << imm;
                }}, imm_type = uint64_t, imm_code = {{ imm = SHAMT6; }});
                0x2: slti({{
                    Rd = (Rs1_sd < imm) ? 1 : 0;
                }});
                0x3: sltiu({{
                    Rd = (Rs1 < imm) ? 1 : 0;
                }}, uint64_t);
                0x4: xori({{
                    Rd = Rs1 ^ imm;
                }}, uint64_t);
                0x5: decode SRTYPE {
                    0x0: srli({{
                        Rd = Rs1 >> imm;
                    }}, imm_type = uint64_t, imm_code = {{ imm = SHAMT6; }});
                    0x1: srai({{
                        Rd_sd = Rs1_sd >> imm;
                    }}, imm_type = uint64_t, imm_code = {{ imm = SHAMT6; }});
                }
                0x6: ori({{
                    Rd = Rs1 | imm;
                }}, uint64_t);
                0x7: andi({{
                    Rd = Rs1 & imm;
                }}, uint64_t);
            }
        }

        0x05: UOp::auipc({{
            Rd = PC + (sext<20>(imm) << 12);
        }});

        0x06: decode FUNCT3 {
            format IOp {
                0x0: addiw({{
                    Rd_sd = Rs1_sw + imm;
                }}, int32_t);
                0x1: slliw({{
                    Rd_sd = Rs1_sw << imm;
                }}, imm_type = uint64_t, imm_code = {{ imm = SHAMT5; }});
                0x5: decode SRTYPE {
                    0x0: srliw({{
                        Rd_sd = (int32_t)(Rs1_uw >> imm);
                    }}, imm_type = uint64_t, imm_code = {{ imm = SHAMT5; }});
                    0x1: sraiw({{
                        Rd_sd = Rs1_sw >> imm;
                    }}, imm_type = uint64_t, imm_code = {{ imm = SHAMT5; }});
                }
            }
        }

        0x08: decode FUNCT3 {
            format Store {
                0x0: sb({{
                    Mem_ub = Rs2_ub;
                }});
                0x1: sh({{
                    Mem_uh = Rs2_uh;
                }});
                0x2: sw({{
                    Mem_uw = Rs2_uw;
                }});
                0x3: sd({{
                    Mem_ud = Rs2_ud;
                }});
            }
        }

        0x09: decode FUNCT3 {
            format Store {
                0x2: fsw({{
                    STATUS status = xc->readMiscReg(MISCREG_STATUS);
                    if (status.fs == FPUStatus::OFF)
                        return std::make_shared<IllegalInstFault>(
                                "FPU is off", machInst);

                    Mem_uw = (uint32_t)Fs2_bits;
                }}, inst_flags=FloatMemWriteOp);
                0x3: fsd({{
                    STATUS status = xc->readMiscReg(MISCREG_STATUS);
                    if (status.fs == FPUStatus::OFF)
                        return std::make_shared<IllegalInstFault>(
                                "FPU is off", machInst);

                    Mem_ud = Fs2_bits;
                }}, inst_flags=FloatMemWriteOp);
            }
            format VectorMemoryStoreOp {
                0x0: decode MOP {
                    0x0: decode SUMOP {
                        0x0: decode NF {
                            0x0: vse8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1: vsseg2e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x2: vsseg3e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3: vsseg4e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x4: vsseg5e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x5: vsseg6e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x6: vsseg7e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7: vsseg8e8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vs1re8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1:vs2re8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3:vs4re8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7:vs8re8_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0xb: vsm_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x10: vse8ff_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vsuxei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsuxseg2ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsuxseg3ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsuxseg4ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsuxseg5ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsuxseg6ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsuxseg7ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsuxseg8ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vsse8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vssseg2e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vssseg3e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vssseg4e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vssseg5e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vssseg6e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vssseg7e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vssseg8e8_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vsoxei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsoxseg2ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsoxseg3ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsoxseg4ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsoxseg5ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsoxseg6ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsoxseg7ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsoxseg8ei8_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                }
                0x5: decode MOP {
                    0x0: decode SUMOP {
                        0x0: decode NF {
                            0x0: vse16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1: vsseg2e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x2: vsseg3e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3: vsseg4e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x4: vsseg5e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x5: vsseg6e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x6: vsseg7e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7: vsseg8e16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vs1re16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1:vs2re16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3:vs4re16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7:vs8re16_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x10: vse16ff_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vsuxei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsuxseg2ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsuxseg3ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsuxseg4ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsuxseg5ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsuxseg6ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsuxseg7ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsuxseg8ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vsse16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vssseg2e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vssseg3e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vssseg4e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vssseg5e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vssseg6e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vssseg7e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vssseg8e16_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vsoxei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsoxseg2ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsoxseg3ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsoxseg4ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsoxseg5ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsoxseg6ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsoxseg7ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsoxseg8ei16_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                }
                0x6: decode MOP {
                    0x0: decode SUMOP {
                        0x0: decode NF {
                            0x0: vse32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1: vsseg2e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x2: vsseg3e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3: vsseg4e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x4: vsseg5e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x5: vsseg6e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x6: vsseg7e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7: vsseg8e32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vs1re32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1:vs2re32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3:vs4re32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7:vs8re32_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x10: vse32ff_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vsuxei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsuxseg2ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsuxseg3ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsuxseg4ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsuxseg5ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsuxseg6ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsuxseg7ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsuxseg8ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vsse32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vssseg2e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vssseg3e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vssseg4e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vssseg5e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vssseg6e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vssseg7e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vssseg8e32_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vsoxei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsoxseg2ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsoxseg3ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsoxseg4ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsoxseg5ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsoxseg6ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsoxseg7ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsoxseg8ei32_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                }
                0x7: decode MOP {
                    0x0: decode SUMOP {
                        0x0: decode NF {
                            0x0: vse64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1: vsseg2e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x2: vsseg3e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3: vsseg4e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x4: vsseg5e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x5: vsseg6e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x6: vsseg7e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7: vsseg8e64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x8: decode NF {
                            0x0:vs1re64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x1:vs2re64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x3:vs4re64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                            0x7:vs8re64_v({{Rs1}},
                                VectorMemoryStoreOp, IsStore, IsVector);
                        }
                        0x10: vse64ff_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x1: decode NF {
                        0x0: vsuxei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsuxseg2ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsuxseg3ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsuxseg4ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsuxseg5ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsuxseg6ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsuxseg7ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsuxseg8ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x2: decode NF {
                        0x0: vsse64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vssseg2e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vssseg3e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vssseg4e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vssseg5e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vssseg6e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vssseg7e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vssseg8e64_v({{Rs1,Rs2}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                    0x3: decode NF {
                        0x0: vsoxei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x1: vsoxseg2ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x2: vsoxseg3ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x3: vsoxseg4ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x4: vsoxseg5ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x5: vsoxseg6ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x6: vsoxseg7ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                        0x7: vsoxseg8ei64_v({{Rs1}},
                            VectorMemoryStoreOp, IsStore, IsVector);
                    }
                }
            }
        }

        0x0b: decode FUNCT3 {
            0x2: decode AMOFUNCT {
                0x2: LoadReserved::lr_w({{
                    Rd_sd = Mem_sw;
                }}, mem_flags=LLSC);
                0x3: StoreCond::sc_w({{
                    Mem_uw = Rs2_uw;
                }}, {{
                    Rd = result;
                }}, inst_flags=IsStoreConditional, mem_flags=LLSC);
                0x0: AtomicMemOp::amoadd_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<int32_t> *amo_op =
                          new AtomicGenericOp<int32_t>(Rs2_sw,
                                  [](int32_t* b, int32_t a){ *b += a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1: AtomicMemOp::amoswap_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x4: AtomicMemOp::amoxor_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b ^= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x8: AtomicMemOp::amoor_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b |= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0xc: AtomicMemOp::amoand_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                          new AtomicGenericOp<uint32_t>(Rs2_uw,
                                  [](uint32_t* b, uint32_t a){ *b &= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x10: AtomicMemOp::amomin_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<int32_t> *amo_op =
                      new AtomicGenericOp<int32_t>(Rs2_sw,
                        [](int32_t* b, int32_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x14: AtomicMemOp::amomax_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<int32_t> *amo_op =
                      new AtomicGenericOp<int32_t>(Rs2_sw,
                        [](int32_t* b, int32_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x18: AtomicMemOp::amominu_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                      new AtomicGenericOp<uint32_t>(Rs2_uw,
                        [](uint32_t* b, uint32_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1c: AtomicMemOp::amomaxu_w({{
                    Rd_sd = Mem_sw;
                }}, {{
                    TypedAtomicOpFunctor<uint32_t> *amo_op =
                      new AtomicGenericOp<uint32_t>(Rs2_uw,
                        [](uint32_t* b, uint32_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
            }
            0x3: decode AMOFUNCT {
                0x2: LoadReserved::lr_d({{
                    Rd_sd = Mem_sd;
                }}, mem_flags=LLSC);
                0x3: StoreCond::sc_d({{
                    Mem = Rs2;
                }}, {{
                    Rd = result;
                }}, mem_flags=LLSC, inst_flags=IsStoreConditional);
                0x0: AtomicMemOp::amoadd_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<int64_t> *amo_op =
                          new AtomicGenericOp<int64_t>(Rs2_sd,
                                  [](int64_t* b, int64_t a){ *b += a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1: AtomicMemOp::amoswap_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                  [](uint64_t* b, uint64_t a){ *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x4: AtomicMemOp::amoxor_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                 [](uint64_t* b, uint64_t a){ *b ^= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x8: AtomicMemOp::amoor_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                 [](uint64_t* b, uint64_t a){ *b |= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0xc: AtomicMemOp::amoand_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                          new AtomicGenericOp<uint64_t>(Rs2_ud,
                                 [](uint64_t* b, uint64_t a){ *b &= a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x10: AtomicMemOp::amomin_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<int64_t> *amo_op =
                      new AtomicGenericOp<int64_t>(Rs2_sd,
                        [](int64_t* b, int64_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x14: AtomicMemOp::amomax_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<int64_t> *amo_op =
                      new AtomicGenericOp<int64_t>(Rs2_sd,
                        [](int64_t* b, int64_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x18: AtomicMemOp::amominu_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                      new AtomicGenericOp<uint64_t>(Rs2_ud,
                        [](uint64_t* b, uint64_t a){ if (a < *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
                0x1c: AtomicMemOp::amomaxu_d({{
                    Rd_sd = Mem_sd;
                }}, {{
                    TypedAtomicOpFunctor<uint64_t> *amo_op =
                      new AtomicGenericOp<uint64_t>(Rs2_ud,
                        [](uint64_t* b, uint64_t a){ if (a > *b) *b = a; });
                }}, mem_flags=ATOMIC_RETURN_OP);
            }
        }
        0x0c: decode FUNCT3 {
            format ROp {
                0x0: decode FUNCT7 {
                    0x0: add({{
                        Rd = Rs1_sd + Rs2_sd;
                    }});
                    0x1: mul({{
                        Rd = Rs1_sd*Rs2_sd;
                    }}, IntMultOp);
                    0x20: sub({{
                        Rd = Rs1_sd - Rs2_sd;
                    }});
                }
                0x1: decode FUNCT7 {
                    0x0: sll({{
                        Rd = Rs1 << Rs2<5:0>;
                    }});
                    0x1: mulh({{
                        bool negate = (Rs1_sd < 0) != (Rs2_sd < 0);

                        uint64_t Rs1_lo = (uint32_t)std::abs(Rs1_sd);
                        uint64_t Rs1_hi = (uint64_t)std::abs(Rs1_sd) >> 32;
                        uint64_t Rs2_lo = (uint32_t)std::abs(Rs2_sd);
                        uint64_t Rs2_hi = (uint64_t)std::abs(Rs2_sd) >> 32;

                        uint64_t hi = Rs1_hi*Rs2_hi;
                        uint64_t mid1 = Rs1_hi*Rs2_lo;
                        uint64_t mid2 = Rs1_lo*Rs2_hi;
                        uint64_t lo = Rs2_lo*Rs1_lo;
                        uint64_t carry = ((uint64_t)(uint32_t)mid1
                                + (uint64_t)(uint32_t)mid2 + (lo >> 32)) >> 32;

                        uint64_t res = hi +
                                       (mid1 >> 32) +
                                       (mid2 >> 32) +
                                       carry;
                        Rd = negate ? ~res + (Rs1_sd*Rs2_sd == 0 ? 1 : 0)
                                    : res;
                    }}, IntMultOp);
                }
                0x2: decode FUNCT7 {
                    0x0: slt({{
                        Rd = (Rs1_sd < Rs2_sd) ? 1 : 0;
                    }});
                    0x1: mulhsu({{
                        bool negate = Rs1_sd < 0;
                        uint64_t Rs1_lo = (uint32_t)std::abs(Rs1_sd);
                        uint64_t Rs1_hi = (uint64_t)std::abs(Rs1_sd) >> 32;
                        uint64_t Rs2_lo = (uint32_t)Rs2;
                        uint64_t Rs2_hi = Rs2 >> 32;

                        uint64_t hi = Rs1_hi*Rs2_hi;
                        uint64_t mid1 = Rs1_hi*Rs2_lo;
                        uint64_t mid2 = Rs1_lo*Rs2_hi;
                        uint64_t lo = Rs1_lo*Rs2_lo;
                        uint64_t carry = ((uint64_t)(uint32_t)mid1
                                + (uint64_t)(uint32_t)mid2 + (lo >> 32)) >> 32;

                        uint64_t res = hi +
                                       (mid1 >> 32) +
                                       (mid2 >> 32) +
                                       carry;
                        Rd = negate ? ~res + (Rs1_sd*Rs2 == 0 ? 1 : 0) : res;
                    }}, IntMultOp);
                }
                0x3: decode FUNCT7 {
                    0x0: sltu({{
                        Rd = (Rs1 < Rs2) ? 1 : 0;
                    }});
                    0x1: mulhu({{
                        uint64_t Rs1_lo = (uint32_t)Rs1;
                        uint64_t Rs1_hi = Rs1 >> 32;
                        uint64_t Rs2_lo = (uint32_t)Rs2;
                        uint64_t Rs2_hi = Rs2 >> 32;

                        uint64_t hi = Rs1_hi*Rs2_hi;
                        uint64_t mid1 = Rs1_hi*Rs2_lo;
                        uint64_t mid2 = Rs1_lo*Rs2_hi;
                        uint64_t lo = Rs1_lo*Rs2_lo;
                        uint64_t carry = ((uint64_t)(uint32_t)mid1
                                + (uint64_t)(uint32_t)mid2 + (lo >> 32)) >> 32;

                        Rd = hi + (mid1 >> 32) + (mid2 >> 32) + carry;
                    }}, IntMultOp);
                }
                0x4: decode FUNCT7 {
                    0x0: xor({{
                        Rd = Rs1 ^ Rs2;
                    }});
                    0x1: div({{
                        if (Rs2_sd == 0) {
                            Rd_sd = -1;
                        } else if (
                                Rs1_sd == std::numeric_limits<int64_t>::min()
                                && Rs2_sd == -1) {
                            Rd_sd = std::numeric_limits<int64_t>::min();
                        } else {
                            Rd_sd = Rs1_sd/Rs2_sd;
                        }
                    }}, IntDivOp);
                }
                0x5: decode FUNCT7 {
                    0x0: srl({{
                        Rd = Rs1 >> Rs2<5:0>;
                    }});
                    0x1: divu({{
                        if (Rs2 == 0) {
                            Rd = std::numeric_limits<uint64_t>::max();
                        } else {
                            Rd = Rs1/Rs2;
                        }
                    }}, IntDivOp);
                    0x20: sra({{
                        Rd_sd = Rs1_sd >> Rs2<5:0>;
                    }});
                }
                0x6: decode FUNCT7 {
                    0x0: or({{
                        Rd = Rs1 | Rs2;
                    }});
                    0x1: rem({{
                        if (Rs2_sd == 0) {
                            Rd = Rs1_sd;
                        } else if (
                                Rs1_sd == std::numeric_limits<int64_t>::min()
                                && Rs2_sd == -1) {
                            Rd = 0;
                        } else {
                            Rd = Rs1_sd%Rs2_sd;
                        }
                    }}, IntDivOp);
                }
                0x7: decode FUNCT7 {
                    0x0: and({{
                        Rd = Rs1 & Rs2;
                    }});
                    0x1: remu({{
                        if (Rs2 == 0) {
                            Rd = Rs1;
                        } else {
                            Rd = Rs1%Rs2;
                        }
                    }}, IntDivOp);
                }
            }
        }

        0x0d: UOp::lui({{
            Rd = (uint64_t)(sext<20>(imm) << 12);
        }});

        0x0e: decode FUNCT3 {
            format ROp {
                0x0: decode FUNCT7 {
                    0x0: addw({{
                        Rd_sd = Rs1_sw + Rs2_sw;
                    }});
                    0x1: mulw({{
                        Rd_sd = (int32_t)(Rs1_sw*Rs2_sw);
                    }}, IntMultOp);
                    0x20: subw({{
                        Rd_sd = Rs1_sw - Rs2_sw;
                    }});
                }
                0x1: sllw({{
                    Rd_sd = Rs1_sw << Rs2<4:0>;
                }});
                0x4: divw({{
                    if (Rs2_sw == 0) {
                        Rd_sd = -1;
                    } else if (Rs1_sw == std::numeric_limits<int32_t>::min()
                            && Rs2_sw == -1) {
                        Rd_sd = std::numeric_limits<int32_t>::min();
                    } else {
                        Rd_sd = Rs1_sw/Rs2_sw;
                    }
                }}, IntDivOp);
                0x5: decode FUNCT7 {
                    0x0: srlw({{
                        Rd_sd = (int32_t)(Rs1_uw >> Rs2<4:0>);
                    }});
                    0x1: divuw({{
                        if (Rs2_uw == 0) {
                            Rd_sd = std::numeric_limits<uint64_t>::max();
                        } else {
                            Rd_sd = (int32_t)(Rs1_uw/Rs2_uw);
                        }
                    }}, IntDivOp);
                    0x20: sraw({{
                        Rd_sd = Rs1_sw >> Rs2<4:0>;
                    }});
                }
                0x6: remw({{
                    if (Rs2_sw == 0) {
                        Rd_sd = Rs1_sw;
                    } else if (Rs1_sw == std::numeric_limits<int32_t>::min()
                            && Rs2_sw == -1) {
                        Rd_sd = 0;
                    } else {
                        Rd_sd = Rs1_sw%Rs2_sw;
                    }
                }}, IntDivOp);
                0x7: remuw({{
                    if (Rs2_uw == 0) {
                        Rd_sd = (int32_t)Rs1_uw;
                    } else {
                        Rd_sd = (int32_t)(Rs1_uw%Rs2_uw);
                    }
                }}, IntDivOp);
            }
        }

        format FPROp {
            0x10: decode FUNCT2 {
                0x0: fmadd_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_mulAdd(f32(freg(Fs1_bits)),
                                         f32(freg(Fs2_bits)),
                                         f32(freg(Fs3_bits))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
                0x1: fmadd_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_mulAdd(f64(freg(Fs1_bits)),
                                         f64(freg(Fs2_bits)),
                                         f64(freg(Fs3_bits))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
            }
            0x11: decode FUNCT2 {
                0x0: fmsub_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_mulAdd(f32(freg(Fs1_bits)),
                                    f32(freg(Fs2_bits)),
                                    f32(f32(freg(Fs3_bits)).v ^
                                        mask(31, 31))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
                0x1: fmsub_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_mulAdd(f64(freg(Fs1_bits)),
                                    f64(freg(Fs2_bits)),
                                    f64(f64(freg(Fs3_bits)).v ^
                                        mask(63, 63))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
            }
            0x12: decode FUNCT2 {
                0x0: fnmsub_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_mulAdd(f32(f32(freg(Fs1_bits)).v ^
                                             mask(31, 31)),
                                         f32(freg(Fs2_bits)),
                                         f32(freg(Fs3_bits))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
                0x1: fnmsub_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_mulAdd(f64(f64(freg(Fs1_bits)).v ^
                                             mask(63, 63)),
                                         f64(freg(Fs2_bits)),
                                         f64(freg(Fs3_bits))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
            }
            0x13: decode FUNCT2 {
                0x0: fnmadd_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_mulAdd(f32(f32(freg(Fs1_bits)).v ^
                                             mask(31, 31)),
                                    f32(freg(Fs2_bits)),
                                    f32(f32(freg(Fs3_bits)).v ^
                                        mask(31, 31))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
                0x1: fnmadd_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_mulAdd(f64(f64(freg(Fs1_bits)).v ^
                                             mask(63, 63)),
                                    f64(freg(Fs2_bits)),
                                    f64(f64(freg(Fs3_bits)).v ^
                                        mask(63, 63))));
                    Fd_bits = fd.v;
                }}, FloatMultAccOp);
            }
            0x14: decode FUNCT7 {
                0x0: fadd_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_add(f32(freg(Fs1_bits)),
                                      f32(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatAddOp);
                0x1: fadd_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_add(f64(freg(Fs1_bits)),
                                      f64(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatAddOp);
                0x4: fsub_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_sub(f32(freg(Fs1_bits)),
                                      f32(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatAddOp);
                0x5: fsub_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_sub(f64(freg(Fs1_bits)),
                                      f64(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatAddOp);
                0x8: fmul_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_mul(f32(freg(Fs1_bits)),
                                      f32(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatMultOp);
                0x9: fmul_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_mul(f64(freg(Fs1_bits)),
                                      f64(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatMultOp);
                0xc: fdiv_s({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_div(f32(freg(Fs1_bits)),
                                      f32(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatDivOp);
                0xd: fdiv_d({{
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_div(f64(freg(Fs1_bits)),
                                      f64(freg(Fs2_bits))));
                    Fd_bits = fd.v;
                }}, FloatDivOp);
                0x10: decode ROUND_MODE {
                    0x0: fsgnj_s({{
                        Fd_bits = boxF32(insertBits(unboxF32(Fs2_bits), 30, 0,
                                                    unboxF32(Fs1_bits)));
                        }}, FloatMiscOp);
                    0x1: fsgnjn_s({{
                        Fd_bits = boxF32(insertBits(unboxF32(~Fs2_bits), 30, 0,
                                                    unboxF32(Fs1_bits)));
                        }}, FloatMiscOp);
                    0x2: fsgnjx_s({{
                        Fd_bits = boxF32(insertBits(
                                    unboxF32(Fs1_bits) ^ unboxF32(Fs2_bits),
                                    30, 0, unboxF32(Fs1_bits)));
                        }}, FloatMiscOp);
                }
                0x11: decode ROUND_MODE {
                    0x0: fsgnj_d({{
                        Fd_bits = insertBits(Fs2_bits, 62, 0, Fs1_bits);
                    }}, FloatMiscOp);
                    0x1: fsgnjn_d({{
                        Fd_bits = insertBits(~Fs2_bits, 62, 0, Fs1_bits);
                    }}, FloatMiscOp);
                    0x2: fsgnjx_d({{
                        Fd_bits = insertBits(
                                Fs1_bits ^ Fs2_bits, 62, 0, Fs1_bits);
                    }}, FloatMiscOp);
                }
                0x14: decode ROUND_MODE {
                    0x0: fmin_s({{
                        bool less = f32_lt_quiet(f32(freg(Fs1_bits)),
                            f32(freg(Fs2_bits))) ||
                            (f32_eq(f32(freg(Fs1_bits)),
                            f32(freg(Fs2_bits))) &&
                            bits(f32(freg(Fs1_bits)).v, 31));

                        Fd_bits = less ||
                            isNaNF32UI(f32(freg(Fs2_bits)).v) ?
                            freg(Fs1_bits).v : freg(Fs2_bits).v;
                        if (isNaNF32UI(f32(freg(Fs1_bits)).v) &&
                            isNaNF32UI(f32(freg(Fs2_bits)).v))
                            Fd_bits = f32(defaultNaNF32UI).v;
                        }}, FloatCmpOp);
                    0x1: fmax_s({{
                        bool greater = f32_lt_quiet(f32(freg(Fs2_bits)),
                            f32(freg(Fs1_bits))) ||
                            (f32_eq(f32(freg(Fs2_bits)),
                            f32(freg(Fs1_bits))) &&
                            bits(f32(freg(Fs2_bits)).v, 31));

                        Fd_bits = greater ||
                            isNaNF32UI(f32(freg(Fs2_bits)).v) ?
                            freg(Fs1_bits).v : freg(Fs2_bits).v;
                        if (isNaNF32UI(f32(freg(Fs1_bits)).v) &&
                            isNaNF32UI(f32(freg(Fs2_bits)).v))
                            Fd_bits = f32(defaultNaNF32UI).v;
                        }}, FloatCmpOp);
                }
                0x15: decode ROUND_MODE {
                    0x0: fmin_d({{
                        bool less = f64_lt_quiet(f64(freg(Fs1_bits)),
                            f64(freg(Fs2_bits))) ||
                            (f64_eq(f64(freg(Fs1_bits)),
                            f64(freg(Fs2_bits))) &&
                            bits(f64(freg(Fs1_bits)).v, 63));

                        Fd_bits = less ||
                            isNaNF64UI(f64(freg(Fs2_bits)).v) ?
                            freg(Fs1_bits).v : freg(Fs2_bits).v;
                        if (isNaNF64UI(f64(freg(Fs1_bits)).v) &&
                            isNaNF64UI(f64(freg(Fs2_bits)).v))
                            Fd_bits = f64(defaultNaNF64UI).v;
                    }}, FloatCmpOp);
                    0x1: fmax_d({{
                        bool greater =
                            f64_lt_quiet(f64(freg(Fs2_bits)),
                            f64(freg(Fs1_bits))) ||
                            (f64_eq(f64(freg(Fs2_bits)),
                            f64(freg(Fs1_bits))) &&
                            bits(f64(freg(Fs2_bits)).v, 63));

                        Fd_bits = greater ||
                            isNaNF64UI(f64(freg(Fs2_bits)).v) ?
                            freg(Fs1_bits).v : freg(Fs2_bits).v;
                        if (isNaNF64UI(f64(freg(Fs1_bits)).v) &&
                            isNaNF64UI(f64(Fs2_bits).v))
                            Fd_bits = f64(defaultNaNF64UI).v;
                    }}, FloatCmpOp);
                }
                0x20: fcvt_s_d({{
                    if (CONV_SGN != 1) {
                        return std::make_shared<IllegalInstFault>(
                                "CONV_SGN != 1", machInst);
                    }
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f64_to_f32(f64(freg(Fs1_bits))));
                    Fd_bits = fd.v;
                }}, FloatCvtOp);
                0x21: fcvt_d_s({{
                    if (CONV_SGN != 0) {
                        return std::make_shared<IllegalInstFault>(
                                "CONV_SGN != 0", machInst);
                    }
                    RM_REQUIRED;
                    freg_t fd;
                    fd = freg(f32_to_f64(f32(freg(Fs1_bits))));
                    Fd_bits = fd.v;
                }}, FloatCvtOp);
                0x2c: fsqrt_s({{
                    if (RS2 != 0) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x1", machInst);
                    }
                    freg_t fd;
                    RM_REQUIRED;
                    fd = freg(f32_sqrt(f32(freg(Fs1_bits))));
                    Fd_bits = fd.v;
                }}, FloatSqrtOp);
                0x2d: fsqrt_d({{
                    if (RS2 != 0) {
                        return std::make_shared<IllegalInstFault>(
                                "source reg x1", machInst);
                    }
                    freg_t fd;
                    RM_REQUIRED;
                    fd = freg(f64_sqrt(f64(freg(Fs1_bits))));
                    Fd_bits = fd.v;
                }}, FloatSqrtOp);
                0x50: decode ROUND_MODE {
                    0x0: fle_s({{
                        Rd = f32_le(f32(freg(Fs1_bits)), f32(freg(Fs2_bits)));
                    }}, FloatCmpOp);
                    0x1: flt_s({{
                        Rd = f32_lt(f32(freg(Fs1_bits)), f32(freg(Fs2_bits)));
                    }}, FloatCmpOp);
                    0x2: feq_s({{
                        Rd = f32_eq(f32(freg(Fs1_bits)), f32(freg(Fs2_bits)));
                    }}, FloatCmpOp);
                }
                0x51: decode ROUND_MODE {
                    0x0: fle_d({{
                        Rd = f64_le(f64(freg(Fs1_bits)), f64(freg(Fs2_bits)));
                    }}, FloatCmpOp);
                    0x1: flt_d({{
                        Rd = f64_lt(f64(freg(Fs1_bits)), f64(freg(Fs2_bits)));
                    }}, FloatCmpOp);
                    0x2: feq_d({{
                        Rd = f64_eq(f64(freg(Fs1_bits)), f64(freg(Fs2_bits)));
                    }}, FloatCmpOp);
                }
                0x60: decode CONV_SGN {
                    0x0: fcvt_w_s({{
                        RM_REQUIRED;
                        Rd_sd = sext<32>(f32_to_i32(f32(freg(Fs1_bits)), rm,
                                                    true));
                    }}, FloatCvtOp);
                    0x1: fcvt_wu_s({{
                        RM_REQUIRED;
                        Rd = sext<32>(f32_to_ui32(f32(freg(Fs1_bits)), rm,
                                                  true));
                    }}, FloatCvtOp);
                    0x2: fcvt_l_s({{
                        RM_REQUIRED;
                        Rd_sd = f32_to_i64(f32(freg(Fs1_bits)), rm, true);
                    }}, FloatCvtOp);
                    0x3: fcvt_lu_s({{
                        RM_REQUIRED;
                        Rd = f32_to_ui64(f32(freg(Fs1_bits)), rm, true);
                    }}, FloatCvtOp);
                }
                0x61: decode CONV_SGN {
                    0x0: fcvt_w_d({{
                        RM_REQUIRED;
                        Rd_sd = sext<32>(f64_to_i32(f64(freg(Fs1_bits)), rm,
                                                    true));
                    }}, FloatCvtOp);
                    0x1: fcvt_wu_d({{
                        RM_REQUIRED;
                        Rd = sext<32>(f64_to_ui32(f64(freg(Fs1_bits)), rm,
                                                  true));
                    }}, FloatCvtOp);
                    0x2: fcvt_l_d({{
                        RM_REQUIRED;
                        Rd_sd = f64_to_i64(f64(freg(Fs1_bits)), rm, true);
                    }}, FloatCvtOp);
                    0x3: fcvt_lu_d({{
                        RM_REQUIRED;
                        Rd = f64_to_ui64(f64(freg(Fs1_bits)), rm, true);
                    }}, FloatCvtOp);
                }
                0x68: decode CONV_SGN {
                    0x0: fcvt_s_w({{
                        RM_REQUIRED;
                        freg_t fd;
                        fd = freg(i32_to_f32((int32_t)Rs1_sw));
                        Fd_bits = fd.v;
                        }}, FloatCvtOp);
                    0x1: fcvt_s_wu({{
                        RM_REQUIRED;
                        freg_t fd;
                        fd = freg(ui32_to_f32((int32_t)Rs1_uw));
                        Fd_bits = fd.v;
                        }}, FloatCvtOp);
                    0x2: fcvt_s_l({{
                        RM_REQUIRED;
                        freg_t fd;
                        fd = freg(i64_to_f32(Rs1_ud));
                        Fd_bits = fd.v;
                        }}, FloatCvtOp);
                    0x3: fcvt_s_lu({{
                        RM_REQUIRED;
                        freg_t fd;
                        fd = freg(ui64_to_f32(Rs1));
                        Fd_bits = fd.v;
                        }}, FloatCvtOp);
                }
                0x69: decode CONV_SGN {
                    0x0: fcvt_d_w({{
                        RM_REQUIRED;
                        Fd = (double)Rs1_sw;
                    }}, FloatCvtOp);
                    0x1: fcvt_d_wu({{
                        RM_REQUIRED;
                        Fd = (double)Rs1_uw;
                    }}, FloatCvtOp);
                    0x2: fcvt_d_l({{
                        RM_REQUIRED;
                        Fd = (double)Rs1_sd;
                    }}, FloatCvtOp);
                    0x3: fcvt_d_lu({{
                        RM_REQUIRED;
                        Fd = (double)Rs1;
                    }}, FloatCvtOp);
                }
                0x70: decode ROUND_MODE {
                    0x0: fmv_x_s({{
                        Rd = (uint32_t)Fs1_bits;
                        if ((Rd&0x80000000) != 0) {
                            Rd |= (0xFFFFFFFFULL << 32);
                        }
                    }}, FloatCvtOp);
                    0x1: fclass_s({{
                        Rd = f32_classify(f32(freg(Fs1_bits)));
                    }}, FloatMiscOp);
                }
                0x71: decode ROUND_MODE {
                    0x0: fmv_x_d({{
                        Rd = freg(Fs1_bits).v;
                    }}, FloatCvtOp);
                    0x1: fclass_d({{
                        Rd = f64_classify(f64(freg(Fs1_bits)));
                    }}, FloatMiscOp);
                }
                0x78: fmv_s_x({{
                    freg_t fd;
                    fd = freg(f32(Rs1_uw));
                    Fd_bits = fd.v;
                }}, FloatCvtOp);
                0x79: fmv_d_x({{
                    freg_t fd;
                    fd = freg(f64(Rs1));
                    Fd_bits = fd.v;
                }}, FloatCvtOp);
            }
        }
        0x15: decode VFUNC_6 {
            0x00: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vadd_vv();
                0x1: VectorArith2SrcOp::vfadd_vv();
                0x2: VectorReductionOp::vredsum_vs();
                0x3: VectorArith2SrcOp::vadd_vi();
                0x4: VectorArith2SrcOp::vadd_vx();
                0x5: VectorArith2SrcOp::vfadd_vf();
                0x7: VectorConfigOp::vsetvli();
            }
            0x01: decode FUNCT3 {
                0x1: VectorReductionOp::vfredsum_vs();
                0x2: VectorReductionOp::vredand_vs();
                }
            0x02: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsub_vv();
                0x1: VectorArith2SrcOp::vfsub_vv();
                0x2: VectorReductionOp::vredor_vs();
                0x3: VectorArith2SrcOp::vsub_vi();
                0x4: VectorArith2SrcOp::vsub_vx();
                0x5: VectorArith2SrcOp::vfsub_vf();
                }
            0x03: decode FUNCT3 {
                0x1: VectorReductionOp::vfredosum_vs();
                0x2: VectorReductionOp::vredxor_vs();
                0x3: VectorArith2SrcOp::vrsub_vi();
                0x4: VectorArith2SrcOp::vrsub_vx();
                }
            0x04: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vminu_vv();
                0x1: VectorArith2SrcOp::vfmin_vv();
                0x2: VectorReductionOp::vredminu_vs();
                0x4: VectorArith2SrcOp::vminu_vx();
                0x5: VectorArith2SrcOp::vfmin_vf();
                }
            0x05: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vmin_vv();
                0x1: VectorReductionOp::vfredmin_vs();
                0x2: VectorReductionOp::vredmin_vs();
                0x4: VectorArith2SrcOp::vmin_vx();
                }
            0x06: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vmaxu_vv();
                0x1: VectorArith2SrcOp::vfmax_vv();
                0x2: VectorReductionOp::vredmaxu_vs();
                0x4: VectorArith2SrcOp::vmaxu_vx();
                0x5: VectorArith2SrcOp::vfmax_vf();
                }
            0x07: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vmax_vv();
                0x1: VectorReductionOp::vfredmax_vs();
                0x2: VectorReductionOp::vredmax_vs();
                0x4: VectorArith2SrcOp::vmax_vx();
                }
            0x08: decode FUNCT3 {
                0x1: VectorArith2SrcOp::vfsgnj_vv();
                0x2: VectorArith2SrcOp::vaaddu_vv();
                0x5: VectorArith2SrcOp::vfsgnj_vf();
                0x6: VectorArith2SrcOp::vaaddu_vx();
                }
            0x09: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vand_vv();
                0x1: VectorArith2SrcOp::vfsgnjn_vv();
                0x2: VectorArith2SrcOp::vaadd_vv();
                0x3: VectorArith2SrcOp::vand_vi();
                0x4: VectorArith2SrcOp::vand_vx();
                0x5: VectorArith2SrcOp::vfsgnjn_vf();
                0x6: VectorArith2SrcOp::vaadd_vx();
                }
            0x0A: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vor_vv();
                0x1: VectorArith2SrcOp::vfsgnjx_vv();
                0x2: VectorArith2SrcOp::vasubu_vv();
                0x3: VectorArith2SrcOp::vor_vi();
                0x4: VectorArith2SrcOp::vor_vx();
                0x5: VectorArith2SrcOp::vfsgnjx_vf();
                0x6: VectorArith2SrcOp::vasubu_vx();
                }
            0x0B: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vxor_vv();
                0x2: VectorArith2SrcOp::vasub_vv();
                0x3: VectorArith2SrcOp::vxor_vi();
                0x4: VectorArith2SrcOp::vxor_vx();
                0x6: VectorArith2SrcOp::vasub_vx();
                }
            0x0C: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vrgather_vv();
                0x3: VectorArith2SrcOp::vrgather_vi();
                0x4: VectorArith2SrcOp::vrgather_vx();
            }
            0x0E: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vrgatherei16_vv();
                0x3: VectorSlideUpOp::vslideup_vi();
                0x4: VectorSlideUpOp::vslideup_vx();
                0x6: VectorSlideUpOp::vslide1up_vx();
                }
            0x0F: decode FUNCT3 {
                0x3: VectorSlideDownOp::vslidedown_vi();
                0x4: VectorSlideDownOp::vslidedown_vx();
                0x6: VectorSlideDownOp::vslide1down_vx();
                }
            0x10: decode FUNCT3 {
                0x0: decode VM {
                    0x0: VectorArith3SrcOp::vadc_vvm();
                }
                0x1: decode VS1 {
                    0x0: decode VM {
                        0x1: VectorToScalarOp::vfmv_fs();
                    }
                }
                0x2: decode VS1 {
                    0x0: decode VM {
                        0x1: VectorToScalarOp::vmv_xs();
                    }
                    0x10: VectorToScalarOp::vcpop_m();
                    0x11: VectorToScalarOp::vfirst_m();
                }
                0x3: decode VM {
                    0x0: VectorArith3SrcOp::vadc_vim();
                }
                0x4: decode VM {
                    0x0: VectorArith3SrcOp::vadc_vxm();
                }
                0x5: decode VS2 {
                    0x0: decode VM {
                        0x1: VectorToScalarOp::vfmv_sf();
                    }
                }
                0x6: decode VS2 {
                    0x0: decode VM {
                        0x1: VectorToScalarOp::vmv_sx();
                    }
                }
            }
            0x11: decode VM {
                0x0: decode FUNCT3 {
                    0x0: VectorArith3SrcOp::vmadc_vvm();
                    0x3: VectorArith3SrcOp::vmadc_vim();
                    0x4: VectorArith3SrcOp::vmadc_vxm();
                    }
                0x1: decode FUNCT3 {
                    0x0: VectorArith2SrcOp::vmadc_vv();
                    0x3: VectorArith2SrcOp::vmadc_vi();
                    0x4: VectorArith2SrcOp::vmadc_vx();
                    }
                }
            0x12: decode VM {
                0x0: decode FUNCT3 {
                    0x0: VectorArith3SrcOp::vsbc_vv();
                    0x2: decode VS1 {
                        0x2: VectorArith1SrcOp::vzext_vf8();
                        0x3: VectorArith1SrcOp::vsext_vf8();
                        0x4: VectorArith1SrcOp::vzext_vf4();
                        0x5: VectorArith1SrcOp::vsext_vf4();
                        0x6: VectorArith1SrcOp::vzext_vf2();
                        0x7: VectorArith1SrcOp::vsext_vf2();
                        }
                    0x4: VectorArith3SrcOp::vsbc_vx();
                    }
                }
            0x13: decode VM {
                0x0: decode FUNCT3 {
                    0x0: VectorArith3SrcOp::vmsbc_vvm();
                    0x4: VectorArith3SrcOp::vmsbc_vxm();
                    }
                0x1: decode FUNCT3 {
                    0x0: VectorArith3SrcOp::vmsbc_vv();
                    0x4: VectorArith3SrcOp::vmsbc_vx();
                    }
                }
            0x14: decode FUNCT3 {
                0x2: decode VS1 {
                    0x1: VectorMaskLogicalOp::vmsbf();
                    0x2: VectorMaskLogicalOp::vmsof();
                    0x3: VectorMaskLogicalOp::vmsif();
                    0x10: VectorMaskLogicalOp::viota();
                    0x11: VectorMaskLogicalOp::vid();
                }
            }
            0x17: decode VM {
                0x0: decode FUNCT3 {
                    0x0: VectorArith2SrcOp::vmerge_vv();
                    0x3: VectorArith2SrcOp::vmerge_vi();
                    0x4: VectorArith2SrcOp::vmerge_vx();
                    0x5: VectorArith2SrcOp::vfmerge_vf();
                    }
                0x1: decode FUNCT3 {
                        0x0: decode VS2 {
                            0x0: VectorArith2SrcOp::vmv_vv();
                            }
                        0x2: VectorArith2SrcOp::vcompress_vv();
                        0x3: decode VS2 {
                            0x0: VectorArith2SrcOp::vmv_vi();
                            }
                        0x4: decode VS2 {
                            0x0: VectorArith2SrcOp::vmv_vx();
                            }
                        0x5: decode VS2 {
                            0x0: VectorArith2SrcOp::vfmv_vf();
                            }
                        }
                    }
            0x18: decode FUNCT3 {
                0x0: VectorIntCompareOp::vmseq_vv();
                0x1: VectorFPCompareOp::vmfeq_vv();
                0x2: VectorMaskLogicalOp::vmandn_mm();
                0x3: VectorIntCompareOp::vmseq_vi();
                0x4: VectorIntCompareOp::vmseq_vx();
                0x5: VectorFPCompareOp::vmfeq_vf();
                }
            0x19: decode FUNCT3 {
                0x0: VectorIntCompareOp::vmsne_vv();
                0x1: VectorFPCompareOp::vmfle_vv();
                0x2: VectorMaskLogicalOp::vmand_mm();
                0x3: VectorIntCompareOp::vmsne_vi();
                0x4: VectorIntCompareOp::vmsne_vx();
                0x5: VectorFPCompareOp::vmfle_vf();
                }
            0x1A: decode FUNCT3 {
                0x0: VectorIntCompareOp::vmsltu_vv();
                0x2: VectorMaskLogicalOp::vmor_mm();
                0x4: VectorIntCompareOp::vmsltu_vx();
                }
            0x1B: decode FUNCT3 {
                0x0: VectorIntCompareOp::vmslt_vv();
                0x1: VectorFPCompareOp::vmflt_vv();
                0x2: VectorMaskLogicalOp::vmxor_mm();
                0x4: VectorIntCompareOp::vmslt_vx();
                0x5: VectorFPCompareOp::vmflt_vf();
                }
            0x1C: decode FUNCT3 {
                0x0: VectorIntCompareOp::vmsleu_vv();
                0x1: VectorFPCompareOp::vmfne_vv();
                0x2: VectorMaskLogicalOp::vmorn_mm();
                0x3: VectorIntCompareOp::vmsleu_vi();
                0x4: VectorIntCompareOp::vmsleu_vx();
                0x5: VectorFPCompareOp::vmfne_vf();
                }
            0x1D: decode FUNCT3 {
                0x0: VectorIntCompareOp::vmsle_vv();
                0x2: VectorMaskLogicalOp::vmnand_mm();
                0x3: VectorIntCompareOp::vmsle_vi();
                0x4: VectorIntCompareOp::vmsle_vx();
                0x5: VectorFPCompareOp::vmfgt_vf();
                }
            0x1E: decode FUNCT3 {
                0x2: VectorMaskLogicalOp::vmnor_mm();
                0x3: VectorIntCompareOp::vmsgtu_vi();
                0x4: VectorIntCompareOp::vmsgtu_vx();
                }
            0x1F: decode FUNCT3 {
                0x2: VectorMaskLogicalOp::vmxnor_mm();
                0x3: VectorIntCompareOp::vmsgt_vi();
                0x4: VectorIntCompareOp::vmsgt_vx();
                0x5: VectorFPCompareOp::vmfge_vf();
                }
            0x20: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsaddu_vv();
                0x1: VectorArith2SrcOp::vfdiv_vv();
                0x2: VectorArith2SrcOp::vdivu_vv();
                0x3: VectorArith2SrcOp::vsaddu_vi();
                0x4: VectorArith2SrcOp::vsaddu_vx();
                0x5: VectorArith2SrcOp::vfdiv_vf();
                0x6: VectorArith2SrcOp::vdivu_vx();
                0x7: VectorConfigOp::vsetvl();
                }
            0x21: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsadd_vv();
                0x2: VectorArith2SrcOp::vdiv_vv();
                0x3: VectorArith2SrcOp::vsadd_vi();
                0x4: VectorArith2SrcOp::vsadd_vx();
                0x6: VectorArith2SrcOp::vdiv_vx();
                }
            0x22: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vssubu_vv();
                0x1: decode VS1{
                    0x0:VectorConvertFPToIntOp::vfcvt_xu_f_v();   // Convert float to unsigned integer.
                    0x1:VectorConvertFPToIntOp::vfcvt_x_f_v();    // Convert float to signed integer.
                    0x2:VectorConvertIntToFPOp::vfcvt_f_xu_v();   // Convert unsigned integer to float.
                    0x3:VectorConvertIntToFPOp::vfcvt_f_x_v();    // Convert signed integer to float.
                    // Widening Operations are not fully supported. WIP
                    0x8:VectorWConvertFPToIntOp::vfwcvt_xu_f_v(); // Convert float to double-width unsigned integer.
                    0x9:VectorWConvertFPToIntOp::vfwcvt_x_f_v();  // Convert float to double-width signed integer.
                    0xA:VectorWConvertIntToFPOp::vfwcvt_f_xu_v(); // Convert unsigned integer to double-width float.
                    0xB:VectorWConvertIntToFPOp::vfwcvt_f_x_v();  // Convert signed integer to double-width float.
                    0xC:VectorWConvertFPToFPOp::vfwcvt_f_f_v();  // Convert single-width float to double-width float.
                    // Narrowing Operations are not fully supported. WIP
                    0x10:VectorNConvertFPToIntOp::vfncvt_xu_f_v(); // Convert double-width float to unsigned integer.
                    0x11:VectorNConvertFPToIntOp::vfncvt_x_f_v();  // Convert double-width float to signed integer.
                    0x12:VectorNConvertIntToFPOp::vfncvt_f_xu_v(); // Convert double-width unsigned integer to float.
                    0x13:VectorNConvertIntToFPOp::vfncvt_f_x_v();  // Convert double-width signed integer to float
                    0x14:VectorNConvertFPToFPOp::vfncvt_f_f_v();  // Convert double-width float to single-width float.
                    }
                0x2: VectorArith2SrcOp::vremu_vv();
                0x4: VectorArith2SrcOp::vssubu_vx();
                0x6: VectorArith2SrcOp::vremu_vx();
                }
            0x23: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vssub_vv();
                0x1: VectorArith1SrcOp::vfsqrt_v();
                0x2: VectorArith2SrcOp::vrem_vv();
                0x4: VectorArith2SrcOp::vssub_vx();
                0x6: VectorArith2SrcOp::vrem_vx();
                }
            0x24: decode FUNCT3 {
                0x1: VectorArith2SrcOp::vfmul_vv();
                0x2: VectorArith2SrcOp::vmulhu_vv();
                0x5: VectorArith2SrcOp::vfmul_vf();
                0x6: VectorArith2SrcOp::vmulhu_vx();
                }
            0x25: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsll_vv();
                0x2: VectorArith2SrcOp::vmul_vv();
                0x3: VectorArith2SrcOp::vsll_vi();
                0x4: VectorArith2SrcOp::vsll_vx();
                0x6: VectorArith2SrcOp::vmul_vx();
                }
            0x26: decode FUNCT3 {
                0x2: VectorArith2SrcOp::vmulhsu_vv();
                0x6: VectorArith2SrcOp::vmulhsu_vx();
            }
            0x27: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsmul_vv();
                0x2: VectorArith2SrcOp::vmulh_vv();
                0x3: decode VM{
                    0x1: decode VIMM5 {
                        0x0: VectorRegisterMoveOp::vmv1r_v();
                        0x1: VectorRegisterMoveOp::vmv2r_v();
                        0x3: VectorRegisterMoveOp::vmv4r_v();
                        0x7: VectorRegisterMoveOp::vmv8r_v();
                        }
                    }
                0x4: VectorArith2SrcOp::vsmul_vx();
                0x6: VectorArith2SrcOp::vmulh_vx();
                }
            0x28: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsrl_vv();
                0x1: VectorArith3SrcOp::vfmadd_vv();
                0x3: VectorArith2SrcOp::vsrl_vi();
                0x4: VectorArith2SrcOp::vsrl_vx();
                0x5: VectorArith3SrcOp::vfmadd_vf();
                }
            0x29: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vsra_vv();
                0x2: VectorArith3SrcOp::vmadd_vv();
                0x3: VectorArith2SrcOp::vsra_vi();
                0x4: VectorArith2SrcOp::vsra_vx();
                0x6: VectorArith3SrcOp::vmadd_vx();
                }
            0x2A: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vssrl_vv();
                0x3: VectorArith2SrcOp::vssrl_vi();
                0x4: VectorArith2SrcOp::vssrl_vx();
                }
            0x2B: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vssra_vv();
                0x2: VectorArith3SrcOp::vnmsub_vv();
                0x3: VectorArith2SrcOp::vssra_vi();
                0x4: VectorArith2SrcOp::vssra_vx();
                0x6: VectorArith3SrcOp::vnmsub_vx();
            }
            0x2C: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vnsrl_wv();
                0x2: VectorArith3SrcOp::vfmacc_vv();
                0x3: VectorArith2SrcOp::vnsrl_wi();
                0x4: VectorArith2SrcOp::vnsrl_wx();
                0x6: VectorArith3SrcOp::vfmacc_vf();
                }
            0x2D: decode FUNCT3 {
                0x0: VectorArith2SrcOp::vnsra_wv();
                0x2: VectorArith3SrcOp::vmacc_vv();
                0x3: VectorArith2SrcOp::vnsra_wi();
                0x4: VectorArith2SrcOp::vnsra_wx();
                0x6: VectorArith3SrcOp::vmacc_vx();
                }
            0x2E: decode FUNCT3 {
                0x0: VectorIntegerWideningOp::vnclipu_wv();
                0x3: VectorIntegerWideningOp::vnclipu_wi();
                0x4: VectorIntegerWideningOp::vnclipu_wx();
            }
            0x2F: decode FUNCT3 {
                0x0: VectorIntegerWideningOp::vnclip_wv();
                0x2: VectorArith3SrcOp::vnmsac_vv();
                0x3: VectorIntegerWideningOp::vnclip_wi();
                0x4: VectorIntegerWideningOp::vnclip_wx();
                0x6: VectorArith3SrcOp::vnmsac_vx();
            }
            0x30: decode FUNCT3 {
                0x0: VectorIntegerWideningOp::vwredsumu_vs();
                0x2: VectorIntegerWideningOp::vwaddu_vv();
                0x6: VectorIntegerWideningOp::vwaddu_vx();
                0x7: VectorConfigOp::vsetivli();
            }
            0x31: decode FUNCT3 {
                0x0: VectorIntegerWideningOp::vwredsum_vs();
                0x2: VectorIntegerWideningOp::vwadd_vv();
                0x6: VectorIntegerWideningOp::vwadd_vx();
            }
            0x32: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwsubu_vv();
                0x6: VectorIntegerWideningOp::vwsubu_vx();
            }
            0x33: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwsub_vv();
                0x6: VectorIntegerWideningOp::vwsub_vx();
            }
            0x34: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwaddu_wv();
                0x6: VectorIntegerWideningOp::vwaddu_wx();
            }
            0x35: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwadd_wv();
                0x6: VectorIntegerWideningOp::vwadd_wx();
            }
            0x36: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwsubu_wv();
                0x6: VectorIntegerWideningOp::vwsubu_wx();
            }
            0x37: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwsub_wv();
                0x6: VectorIntegerWideningOp::vwsub_wx();
            }
            0x38: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwmulu_vv();
                0x6: VectorIntegerWideningOp::vwmulu_vx();
            }
            0x3A: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwmulsu_vv();
                0x6: VectorIntegerWideningOp::vwmulsu_vx();
            }
            0x3B: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwmul_vv();
                0x6: VectorIntegerWideningOp::vwmul_vx();
            }
            0x3C: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwmaccu_vv();
                0x6: VectorIntegerWideningOp::vwmaccu_vx();
            }
            0x3D: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwmacc_vv();
                0x6: VectorIntegerWideningOp::vwmacc_vx();
            }
            0x3E: decode FUNCT3 {
                0x6: VectorIntegerWideningOp::vwmaccus_vx();
            }
            0x3F: decode FUNCT3 {
                0x2: VectorIntegerWideningOp::vwmaccsu_vv();
                0x6: VectorIntegerWideningOp::vwmaccsu_vx();
            }
        }
        0x18: decode FUNCT3 {
            format BOp {
                0x0: beq({{
                    if (Rs1 == Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x1: bne({{
                    if (Rs1 != Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x4: blt({{
                    if (Rs1_sd < Rs2_sd) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x5: bge({{
                    if (Rs1_sd >= Rs2_sd) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x6: bltu({{
                    if (Rs1 < Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
                0x7: bgeu({{
                    if (Rs1 >= Rs2) {
                        NPC = PC + imm;
                    } else {
                        NPC = NPC;
                    }
                }}, IsDirectControl, IsCondControl);
            }
        }

        0x19: decode FUNCT3 {
            0x0: Jump::jalr({{
                Rd = NPC;
                NPC = (imm + Rs1) & (~0x1);
            }}, IsIndirectControl, IsUncondControl, IsCall);
        }

        0x1b: JOp::jal({{
            Rd = NPC;
            NPC = PC + imm;
        }}, IsDirectControl, IsUncondControl, IsCall);

        0x1c: decode FUNCT3 {
            format SystemOp {
                0x0: decode FUNCT7 {
                    0x0: decode RS2 {
                        0x0: ecall({{
                            return std::make_shared<SyscallFault>(
                                (PrivilegeMode)xc->readMiscReg(MISCREG_PRV));
                        }}, IsSerializeAfter, IsNonSpeculative, IsSyscall,
                            No_OpClass);
                        0x1: ebreak({{
                            return std::make_shared<BreakpointFault>(
                                xc->pcState());
                        }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                        0x2: uret({{
                            STATUS status = xc->readMiscReg(MISCREG_STATUS);
                            status.uie = status.upie;
                            status.upie = 1;
                            xc->setMiscReg(MISCREG_STATUS, status);
                            NPC = xc->readMiscReg(MISCREG_UEPC);
                        }}, IsSerializeAfter, IsNonSpeculative, IsReturn);
                    }
                    0x8: decode RS2 {
                        0x2: sret({{
                            STATUS status = xc->readMiscReg(MISCREG_STATUS);
                            auto pm = (PrivilegeMode)xc->readMiscReg(
                                MISCREG_PRV);
                            if (pm == PRV_U ||
                                (pm == PRV_S && status.tsr == 1)) {
                                return std::make_shared<IllegalInstFault>(
                                            "sret in user mode or TSR enabled",
                                            machInst);
                                NPC = NPC;
                            } else {
                                xc->setMiscReg(MISCREG_PRV, status.spp);
                                status.sie = status.spie;
                                status.spie = 1;
                                status.spp = PRV_U;
                                xc->setMiscReg(MISCREG_STATUS, status);
                                NPC = xc->readMiscReg(MISCREG_SEPC);
                            }
                        }}, IsSerializeAfter, IsNonSpeculative, IsReturn);
                        0x5: wfi({{
                            STATUS status = xc->readMiscReg(MISCREG_STATUS);
                            auto pm = (PrivilegeMode)xc->readMiscReg(
                                MISCREG_PRV);
                            if (pm == PRV_U ||
                                (pm == PRV_S && status.tw == 1)) {
                                return std::make_shared<IllegalInstFault>(
                                            "wfi in user mode or TW enabled",
                                            machInst);
                            }
                            // don't do anything for now
                        }}, No_OpClass);
                    }
                    0x9: sfence_vma({{
                        STATUS status = xc->readMiscReg(MISCREG_STATUS);
                        auto pm = (PrivilegeMode)xc->readMiscReg(MISCREG_PRV);
                        if (pm == PRV_U || (pm == PRV_S && status.tvm == 1)) {
                            return std::make_shared<IllegalInstFault>(
                                        "sfence in user mode or TVM enabled",
                                        machInst);
                        }
                        xc->tcBase()->getMMUPtr()->demapPage(Rs1, Rs2);
                    }}, IsNonSpeculative, IsSerializeAfter, No_OpClass);
                    0x18: mret({{
                        if (xc->readMiscReg(MISCREG_PRV) != PRV_M) {
                            return std::make_shared<IllegalInstFault>(
                                        "mret at lower privilege", machInst);
                            NPC = NPC;
                        } else {
                            STATUS status = xc->readMiscReg(MISCREG_STATUS);
                            xc->setMiscReg(MISCREG_PRV, status.mpp);
                            xc->setMiscReg(MISCREG_NMIE, 1);
                            status.mie = status.mpie;
                            status.mpie = 1;
                            status.mpp = PRV_U;
                            xc->setMiscReg(MISCREG_STATUS, status);
                            NPC = xc->readMiscReg(MISCREG_MEPC);
                        }
                    }}, IsSerializeAfter, IsNonSpeculative, IsReturn);
                }
            }
            format CSROp {
                0x1: csrrw({{
                    Rd = data;
                    data = Rs1;
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                0x2: csrrs({{
                    Rd = data;
                    data |= Rs1;
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                0x3: csrrc({{
                    Rd = data;
                    data &= ~Rs1;
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                0x5: csrrwi({{
                    Rd = data;
                    data = uimm;
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                0x6: csrrsi({{
                    Rd = data;
                    data |= uimm;
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
                0x7: csrrci({{
                    Rd = data;
                    data &= ~uimm;
                }}, IsSerializeAfter, IsNonSpeculative, No_OpClass);
            }
        }

        0x1e: M5Op::M5Op();
    }
}
